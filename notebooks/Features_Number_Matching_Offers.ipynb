{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_name = 'transactions.csv'\n",
    "chunksize = 10 ** 6\n",
    "chunk_iter = pd.read_csv(file_name, chunksize=chunksize)\n",
    "chunk1 = chunk_iter.next()\n",
    "chunk1.to_csv('transactions_chunk1.csv', index=False)\n",
    "lines = sc.textFile('transactions_chunk1.csv')\n",
    "header = lines.first()\n",
    "lines = lines.filter(lambda row: row != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Functions for parsing the strings from the CSV file\n",
    "\n",
    "def toIntSafe(inval):\n",
    "  try:\n",
    "    return int(inval)\n",
    "  except ValueError:\n",
    "    return None\n",
    "\n",
    "def toTimeSafe(inval):\n",
    "  try:\n",
    "    return datetime.strptime(inval, \"%Y-%m-%d\")\n",
    "  except ValueError:\n",
    "    return None\n",
    "\n",
    "def toLongSafe(inval):\n",
    "  try:\n",
    "    return long(inval)\n",
    "  except ValueError:\n",
    "    return None\n",
    "\n",
    "def toFloatSafe(inval):\n",
    "  try:\n",
    "    return float(inval)\n",
    "  except ValueError:\n",
    "    return None\n",
    "    \n",
    "def stringToPost(row):\n",
    "  r = row.encode('utf8').split(',')\n",
    "  return Row(\n",
    "    int(r[0]),         # Don't want this column to be nullable\n",
    "    toLongSafe(r[1]),\n",
    "    toLongSafe(r[2]),\n",
    "    toLongSafe(r[3]),\n",
    "    toLongSafe(r[4]),\n",
    "    toLongSafe(r[5]),\n",
    "    toTimeSafe(r[6]),\n",
    "    toFloatSafe(r[7]),\n",
    "    r[8],\n",
    "    toLongSafe(r[9]),\n",
    "    toFloatSafe(r[10]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transactions_schema = StructType([\n",
    "    StructField(\"id\", LongType(), False),\n",
    "    StructField(\"chain\", LongType(), True),\n",
    "    StructField(\"dept\", LongType(), True),\n",
    "    StructField(\"category\", LongType(), True),\n",
    "    StructField(\"company\", LongType(), True),\n",
    "    StructField(\"brand\", LongType(), True),\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"productsize\", DoubleType(), True),\n",
    "    StructField(\"purchasemeasure\", StringType(), True),\n",
    "    StructField(\"purchasequantity\", LongType(), True),\n",
    "    StructField(\"purchaseamount\", DoubleType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rowRDD = lines.map(lambda p: stringToPost(p))\n",
    "transactions = sqlContext.createDataFrame(rowRDD, transactions_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+--------+----------+-----+----------+-----------+---------------+----------------+--------------+\n",
      "|   id|chain|dept|category|   company|brand|      date|productsize|purchasemeasure|purchasequantity|purchaseamount|\n",
      "+-----+-----+----+--------+----------+-----+----------+-----------+---------------+----------------+--------------+\n",
      "|86246|  205|   7|     707|1078778070|12564|2012-03-02|       12.0|             OZ|               1|          7.59|\n",
      "|86246|  205|  63|    6319| 107654575|17876|2012-03-02|       64.0|             OZ|               1|          1.59|\n",
      "|86246|  205|  97|    9753|1022027929|    0|2012-03-02|        1.0|             CT|               1|          5.99|\n",
      "|86246|  205|  25|    2509| 107996777|31373|2012-03-02|       16.0|             OZ|               1|          1.99|\n",
      "|86246|  205|  55|    5555| 107684070|32094|2012-03-02|       16.0|             OZ|               2|         10.38|\n",
      "|86246|  205|  97|    9753|1021015020|    0|2012-03-02|        1.0|             CT|               1|           7.8|\n",
      "|86246|  205|  99|    9909| 104538848|15343|2012-03-02|       16.0|             OZ|               1|          2.49|\n",
      "|86246|  205|  59|    5907| 102900020| 2012|2012-03-02|       16.0|             OZ|               1|          1.39|\n",
      "|86246|  205|   9|     921| 101128414| 9209|2012-03-02|        4.0|             OZ|               2|           1.5|\n",
      "|86246|  205|  73|    7344|1068142161|20285|2012-03-02|        8.0|             CT|               1|          5.79|\n",
      "|86246|  205|  41|    4107| 104113040|28204|2012-03-02|       14.5|             OZ|               1|          0.59|\n",
      "|86246|  205|  21|    2106| 105100050|27873|2012-03-02|       64.0|             OZ|               1|          3.29|\n",
      "|86246|  205|   8|     814| 102840020|18584|2012-03-02|       15.5|             OZ|               1|          3.29|\n",
      "|86246|  205|  91|    9122| 108200080| 2911|2012-03-02|       10.0|             OZ|               1|          1.99|\n",
      "|86246|  205|  41|    4120| 101116616|15266|2012-03-02|        6.0|             OZ|               1|          0.89|\n",
      "|86246|  205|  63|    6315| 107996777|31373|2012-03-02|       64.0|             OZ|               1|          3.59|\n",
      "|86246|  205|   9|     907| 101410010|13791|2012-03-02|       24.0|             OZ|               1|          3.99|\n",
      "|86246|  205|  97|    9753|1021013323|    0|2012-03-02|        1.0|             CT|               1|          8.87|\n",
      "|86246|  205|  45|    4509|1082650484|59628|2012-03-02|       16.0|             OZ|               1|          4.99|\n",
      "|86246|  205|  26|    2630| 103700030|14647|2012-03-02|       56.0|             CT|               1|           1.0|\n",
      "+-----+-----+----+--------+----------+-----+----------+-----------+---------------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('trainHistory.csv')\n",
    "test = pd.read_csv('testHistory.csv')\n",
    "offers = pd.read_csv('offers.csv')\n",
    "# Add department to offers\n",
    "dept = []\n",
    "for i in range(offers.shape[0]):\n",
    "    str_category = str(offers['category'].iloc[i])\n",
    "    if len(str_category) == 4:\n",
    "        dept.append(str_category[:2])\n",
    "    else:\n",
    "        dept.append(str_category[:1])\n",
    "        \n",
    "offers['dept'] = dept\n",
    "offers['dept'] = pd.to_numeric(offers['dept'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reencode the target variable\n",
    "train['repeater'] = np.where(train['repeater'] == 't', '1', '0')\n",
    "train['repeater'] = pd.to_numeric(train['repeater'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.merge(train, offers, how='left', on=['offer'])\n",
    "test = pd.merge(test, offers, how='left', on=['offer'])\n",
    "# Rename columns\n",
    "train.columns = ['id', 'chain', 'offer', 'market', 'repeattrips', 'repeater', 'offerdate', \n",
    "                 'offer_category', 'quantity', 'offer_company', 'offervalue', 'offer_brand',\n",
    "                 'offer_dept']\n",
    "test.columns = ['id', 'chain', 'offer', 'market', 'offerdate', 'offer_category', 'quantity', \n",
    "                'offer_company', 'offervalue', 'offer_brand', 'offer_dept']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_schema = StructType([\n",
    "    StructField(\"id\", LongType(), False),\n",
    "    StructField(\"offer_chain\", LongType(), True),\n",
    "    StructField(\"offer\", LongType(), True),\n",
    "    StructField(\"market\", LongType(), True),\n",
    "    StructField(\"repeattrips\", LongType(), True),\n",
    "    StructField(\"repeater\", IntegerType(), True),\n",
    "    StructField(\"offerdate\", StringType(), True),\n",
    "    StructField(\"offer_category\", LongType(), True),\n",
    "    StructField(\"quantity\", LongType(), True),\n",
    "    StructField(\"offer_company\", LongType(), True),\n",
    "    StructField(\"offervalue\", DoubleType(), True),\n",
    "    StructField(\"offer_brand\", LongType(), True),\n",
    "    StructField(\"offer_dept\", LongType(), True)\n",
    "    ])\n",
    "\n",
    "train_df = sqlContext.createDataFrame(train, train_schema)\n",
    "test_schema = StructType([\n",
    "    StructField(\"id\", LongType(), False),\n",
    "    StructField(\"offer_chain\", LongType(), True),\n",
    "    StructField(\"offer\", LongType(), True),\n",
    "    StructField(\"market\", LongType(), True),\n",
    "    StructField(\"offerdate\", StringType(), True),\n",
    "    StructField(\"offer_category\", LongType(), True),\n",
    "    StructField(\"quantity\", LongType(), True),\n",
    "    StructField(\"offer_company\", LongType(), True),\n",
    "    StructField(\"offervalue\", DoubleType(), True),\n",
    "    StructField(\"offer_brand\", LongType(), True),\n",
    "    StructField(\"offer_dept\", LongType(), True)\n",
    "    ])\n",
    "test_df = sqlContext.createDataFrame(test, test_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chain = train_df.join(transactions, (train_df.id == transactions.id) & (train_df.offer_chain == transactions.chain), \n",
    "                          \"left\").select(train_df[\"id\"],\"offer_chain\",\"chain\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chain = chain.withColumn(\"matchChainCount\", F.expr(\"case when offer_chain = chain then 1 else 0 end\"))\n",
    "chain = chain.groupBy(\"id\").sum(\"matchChainCount\").withColumnRenamed(\"sum(matchChainCount)\", \"matchChainCount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dept = train_df.join(transactions, (train_df.id == transactions.id) & (train_df.offer_dept == transactions.dept), \n",
    "                          \"left\").select(train_df[\"id\"],\"offer_dept\",\"dept\")\n",
    "dept = dept.withColumn(\"matchDeptCount\", F.expr(\"case when offer_dept = dept then 1 else 0 end\"))\n",
    "dept = dept.groupBy(\"id\").sum(\"matchDeptCount\").withColumnRenamed(\"sum(matchDeptCount)\", \"matchDeptCount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "category = train_df.join(transactions, (train_df.id == transactions.id) & (train_df.offer_category == transactions.category), \n",
    "                          \"left\").select(train_df[\"id\"],\"offer_category\",\"category\")\n",
    "category = category.withColumn(\"matchCategoryCount\", F.expr(\"case when offer_category = category then 1 else 0 end\"))\n",
    "category = category.groupBy(\"id\").sum(\"matchCategoryCount\").withColumnRenamed(\"sum(matchCategoryCount)\", \"matchCategoryCount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "company = train_df.join(transactions, (train_df.id == transactions.id) & (train_df.offer_company == transactions.company), \n",
    "                          \"left\").select(train_df[\"id\"],\"offer_company\",\"company\")\n",
    "company = company.withColumn(\"matchCompanyCount\", F.expr(\"case when offer_company = company then 1 else 0 end\"))\n",
    "company = company.groupBy(\"id\").sum(\"matchCompanyCount\").withColumnRenamed(\"sum(matchCompanyCount)\", \"matchCompanyCount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "brand = train_df.join(transactions, (train_df.id == transactions.id) & (train_df.offer_brand == transactions.brand), \n",
    "                          \"left\").select(train_df[\"id\"],\"offer_brand\",\"brand\")\n",
    "brand = brand.withColumn(\"matchBrandCount\", F.expr(\"case when offer_brand = brand then 1 else 0 end\"))\n",
    "brand = brand.groupBy(\"id\").sum(\"matchBrandCount\").withColumnRenamed(\"sum(matchBrandCount)\", \"matchBrandCount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "repeats = train_df.select(\"id\", \"repeater\")\n",
    "match = chain.join(dept, \"id\", \"left\").join(category, \"id\", \"left\").join(company, \"id\", \"left\").join(brand, \"id\", \"left\").join(repeats, \"id\", \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "match.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "match = match.withColumnRenamed(\"repeater\", \"label\")\n",
    "va = VectorAssembler(outputCol=\"features\", inputCols = match.columns[1:6])\n",
    "\n",
    "penlpoints = va.transform(match).select(\"features\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|       features|repeater|\n",
      "+---------------+--------+\n",
      "|(5,[0],[387.0])|       0|\n",
      "|      (5,[],[])|       0|\n",
      "|      (5,[],[])|       1|\n",
      "|      (5,[],[])|       0|\n",
      "|      (5,[],[])|       0|\n",
      "|      (5,[],[])|       0|\n",
      "|      (5,[],[])|       0|\n",
      "|      (5,[],[])|       0|\n",
      "|      (5,[],[])|       0|\n",
      "|      (5,[],[])|       1|\n",
      "|      (5,[],[])|       0|\n",
      "|      (5,[],[])|       0|\n",
      "|      (5,[],[])|       0|\n",
      "|      (5,[],[])|       1|\n",
      "|      (5,[],[])|       1|\n",
      "|      (5,[],[])|       1|\n",
      "|      (5,[],[])|       1|\n",
      "|      (5,[],[])|       0|\n",
      "|      (5,[],[])|       1|\n",
      "|      (5,[],[])|       0|\n",
      "+---------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "penlpoints.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test and Transaction\n",
    "\n",
    "test_chain = test_df.join(transactions, (test_df.id == transactions.id) & (test_df.offer_chain == transactions.chain), \n",
    "                          \"left\").select(test_df[\"id\"],\"offer_chain\",\"chain\")\n",
    "test_chain = test_chain.withColumn(\"matchChainCount\", F.expr(\"case when offer_chain = chain then 1 else 0 end\"))\n",
    "test_chain = test_chain.groupBy(\"id\").sum(\"matchChainCount\").withColumnRenamed(\"sum(matchChainCount)\", \"matchChainCount\")\n",
    "\n",
    "test_dept = test_df.join(transactions, (test_df.id == transactions.id) & (test_df.offer_dept == transactions.dept), \n",
    "                          \"left\").select(test_df[\"id\"],\"offer_dept\",\"dept\")\n",
    "test_dept = test_dept.withColumn(\"matchDeptCount\", F.expr(\"case when offer_dept = dept then 1 else 0 end\"))\n",
    "test_dept = test_dept.groupBy(\"id\").sum(\"matchDeptCount\").withColumnRenamed(\"sum(matchDeptCount)\", \"matchDeptCount\")\n",
    "\n",
    "test_category = test_df.join(transactions, (test_df.id == transactions.id) & (test_df.offer_category == transactions.category), \n",
    "                          \"left\").select(test_df[\"id\"],\"offer_category\",\"category\")\n",
    "test_category = test_category.withColumn(\"matchCategoryCount\", F.expr(\"case when offer_category = category then 1 else 0 end\"))\n",
    "test_category = test_category.groupBy(\"id\").sum(\"matchCategoryCount\").withColumnRenamed(\"sum(matchCategoryCount)\", \"matchCategoryCount\")\n",
    "\n",
    "test_company = test_df.join(transactions, (test_df.id == transactions.id) & (test_df.offer_company == transactions.company), \n",
    "                          \"left\").select(test_df[\"id\"],\"offer_company\",\"company\")\n",
    "test_company = test_company.withColumn(\"matchCompanyCount\", F.expr(\"case when offer_company = company then 1 else 0 end\"))\n",
    "test_company = test_company.groupBy(\"id\").sum(\"matchCompanyCount\").withColumnRenamed(\"sum(matchCompanyCount)\", \"matchCompanyCount\")\n",
    "\n",
    "test_brand = test_df.join(transactions, (test_df.id == transactions.id) & (test_df.offer_brand == transactions.brand), \n",
    "                          \"left\").select(test_df[\"id\"],\"offer_brand\",\"brand\")\n",
    "test_brand = test_brand.withColumn(\"matchBrandCount\", F.expr(\"case when offer_brand = brand then 1 else 0 end\"))\n",
    "test_brand = test_brand.groupBy(\"id\").sum(\"matchBrandCount\").withColumnRenamed(\"sum(matchBrandCount)\", \"matchBrandCount\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_match = test_chain.join(test_dept, \"id\", \"left\").join(test_category, \"id\", \"left\").join(test_company, \"id\", \"left\").join(test_brand, \"id\", \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "va = VectorAssembler(outputCol=\"features\", inputCols = test_match.columns[1:6])\n",
    "testpoints = va.transform(test_match).select(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lrmocel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-184-788552ba5363>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregParam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxIter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitIntercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlrmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpenlpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlrmocel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lrmocel' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(regParam=0.01, maxIter=1000, fitIntercept=True)\n",
    "lrmodel = lr.fit(penlpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = lrmodel.transform(testpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+\n",
      "|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "|[517.0,0.0,0.0,2....|[0.87300474302017...|[0.70537053764279...|       0.0|\n",
      "|[419.0,6.0,2.0,4....|[1.01912578103353...|[0.73480227723325...|       0.0|\n",
      "|[710.0,93.0,54.0,...|[3.07034757406588...|[0.95565290508235...|       0.0|\n",
      "|           (5,[],[])|[0.98770750770535...|[0.72863487446255...|       0.0|\n",
      "|           (5,[],[])|[0.98770750770535...|[0.72863487446255...|       0.0|\n",
      "|           (5,[],[])|[0.98770750770535...|[0.72863487446255...|       0.0|\n",
      "|           (5,[],[])|[0.98770750770535...|[0.72863487446255...|       0.0|\n",
      "|           (5,[],[])|[0.98770750770535...|[0.72863487446255...|       0.0|\n",
      "|           (5,[],[])|[0.98770750770535...|[0.72863487446255...|       0.0|\n",
      "|           (5,[],[])|[0.98770750770535...|[0.72863487446255...|       0.0|\n",
      "|           (5,[],[])|[0.98770750770535...|[0.72863487446255...|       0.0|\n",
      "|           (5,[],[])|[0.98770750770535...|[0.72863487446255...|       0.0|\n",
      "|           (5,[],[])|[0.98770750770535...|[0.72863487446255...|       0.0|\n",
      "|           (5,[],[])|[0.98770750770535...|[0.72863487446255...|       0.0|\n",
      "|           (5,[],[])|[0.98770750770535...|[0.72863487446255...|       0.0|\n",
      "|           (5,[],[])|[0.98770750770535...|[0.72863487446255...|       0.0|\n",
      "|           (5,[],[])|[0.98770750770535...|[0.72863487446255...|       0.0|\n",
      "|           (5,[],[])|[0.98770750770535...|[0.72863487446255...|       0.0|\n",
      "|           (5,[],[])|[0.98770750770535...|[0.72863487446255...|       0.0|\n",
      "|           (5,[],[])|[0.98770750770535...|[0.72863487446255...|       0.0|\n",
      "+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(features=DenseVector([517.0, 0.0, 0.0, 2.0, 2.0]), rawPrediction=DenseVector([0.873, -0.873]), probability=DenseVector([0.7054, 0.2946]), prediction=0.0)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_prob = pred.select('probability').toPandas()\n",
    "prob = []\n",
    "for i in range(test_prob.shape[0]):\n",
    "    prob.append(test_prob['probability'][i][1])\n",
    "test_prob['repeatProbability'] = prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_id = test_df.select(\"id\").toPandas()\n",
    "test_final_pred = pd.merge(test_id, test_prob, left_index=True, right_index=True)\n",
    "test_final_pred.drop('probability', 1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probability</th>\n",
       "      <th>repeatProbability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.705370537643, 0.294629462357]</td>\n",
       "      <td>0.294629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.734802277233, 0.265197722767]</td>\n",
       "      <td>0.265198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.955652905082, 0.0443470949176]</td>\n",
       "      <td>0.044347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.728634874463, 0.271365125537]</td>\n",
       "      <td>0.271365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.728634874463, 0.271365125537]</td>\n",
       "      <td>0.271365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.728634874463, 0.271365125537]</td>\n",
       "      <td>0.271365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.728634874463, 0.271365125537]</td>\n",
       "      <td>0.271365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.728634874463, 0.271365125537]</td>\n",
       "      <td>0.271365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.728634874463, 0.271365125537]</td>\n",
       "      <td>0.271365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.728634874463, 0.271365125537]</td>\n",
       "      <td>0.271365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         probability  repeatProbability\n",
       "0   [0.705370537643, 0.294629462357]           0.294629\n",
       "1   [0.734802277233, 0.265197722767]           0.265198\n",
       "2  [0.955652905082, 0.0443470949176]           0.044347\n",
       "3   [0.728634874463, 0.271365125537]           0.271365\n",
       "4   [0.728634874463, 0.271365125537]           0.271365\n",
       "5   [0.728634874463, 0.271365125537]           0.271365\n",
       "6   [0.728634874463, 0.271365125537]           0.271365\n",
       "7   [0.728634874463, 0.271365125537]           0.271365\n",
       "8   [0.728634874463, 0.271365125537]           0.271365\n",
       "9   [0.728634874463, 0.271365125537]           0.271365"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_final_pred.head(10)\n",
    "test_prob.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_final_pred.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
