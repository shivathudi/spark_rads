{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime, date\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip this cell if you already have a chunk of the transactions data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name = '../data/transactions.csv'\n",
    "chunksize = 10 ** 5\n",
    "chunk_iter = pd.read_csv(file_name, chunksize=chunksize)\n",
    "chunk1 = chunk_iter.next()\n",
    "chunk1.to_csv('../data/transactions_chunk1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile('../data/transactions_chunk1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "header = lines.first()\n",
    "lines = lines.filter(lambda row: row != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions for parsing the strings from the CSV file\n",
    "\n",
    "def toIntSafe(inval):\n",
    "  try:\n",
    "    return int(inval)\n",
    "  except ValueError:\n",
    "    return None\n",
    "\n",
    "def toTimeSafe(inval):\n",
    "  try:\n",
    "    return datetime.strptime(inval, \"%Y-%m-%d\")\n",
    "  except ValueError:\n",
    "    return None\n",
    "\n",
    "def toLongSafe(inval):\n",
    "  try:\n",
    "    return long(inval)\n",
    "  except ValueError:\n",
    "    return None\n",
    "\n",
    "def toFloatSafe(inval):\n",
    "  try:\n",
    "    return float(inval)\n",
    "  except ValueError:\n",
    "    return None\n",
    "    \n",
    "def stringToPost(row):\n",
    "  r = row.encode('utf8').split(',')\n",
    "  return Row(\n",
    "    int(r[0]),         # Don't want this column to be nullable\n",
    "    toLongSafe(r[1]),\n",
    "    toLongSafe(r[2]),\n",
    "    toLongSafe(r[3]),\n",
    "    toLongSafe(r[4]),\n",
    "    toLongSafe(r[5]),\n",
    "    toTimeSafe(r[6]),\n",
    "    toFloatSafe(r[7]),\n",
    "    r[8],\n",
    "    toLongSafe(r[9]),\n",
    "    toFloatSafe(r[10]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transactions_schema = StructType([\n",
    "    StructField(\"id\", LongType(), False),\n",
    "    StructField(\"chain\", LongType(), True),\n",
    "    StructField(\"dept\", LongType(), True),\n",
    "    StructField(\"category\", LongType(), True),\n",
    "    StructField(\"company\", LongType(), True),\n",
    "    StructField(\"brand\", LongType(), True),\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"productsize\", DoubleType(), True),\n",
    "    StructField(\"purchasemeasure\", StringType(), True),\n",
    "    StructField(\"purchasequantity\", LongType(), True),\n",
    "    StructField(\"purchaseamount\", DoubleType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rowRDD = lines.map(lambda p: stringToPost(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transactions = sqlContext.createDataFrame(rowRDD, transactions_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u'Table `transactions` already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-f5a2202941c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'transactions'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/sql/readwriter.pyc\u001b[0m in \u001b[0;36msaveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u'Table `transactions` already exists.;'"
     ]
    }
   ],
   "source": [
    "transactions.write.saveAsTable('transactions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- chain: long (nullable = true)\n",
      " |-- dept: long (nullable = true)\n",
      " |-- category: long (nullable = true)\n",
      " |-- company: long (nullable = true)\n",
      " |-- brand: long (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- productsize: double (nullable = true)\n",
      " |-- purchasemeasure: string (nullable = true)\n",
      " |-- purchasequantity: long (nullable = true)\n",
      " |-- purchaseamount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+--------+----------+-----+----------+-----------+---------------+----------------+--------------+\n",
      "|   id|chain|dept|category|   company|brand|      date|productsize|purchasemeasure|purchasequantity|purchaseamount|\n",
      "+-----+-----+----+--------+----------+-----+----------+-----------+---------------+----------------+--------------+\n",
      "|86246|  205|   7|     707|1078778070|12564|2012-03-02|       12.0|             OZ|               1|          7.59|\n",
      "|86246|  205|  63|    6319| 107654575|17876|2012-03-02|       64.0|             OZ|               1|          1.59|\n",
      "|86246|  205|  97|    9753|1022027929|    0|2012-03-02|        1.0|             CT|               1|          5.99|\n",
      "|86246|  205|  25|    2509| 107996777|31373|2012-03-02|       16.0|             OZ|               1|          1.99|\n",
      "|86246|  205|  55|    5555| 107684070|32094|2012-03-02|       16.0|             OZ|               2|         10.38|\n",
      "|86246|  205|  97|    9753|1021015020|    0|2012-03-02|        1.0|             CT|               1|           7.8|\n",
      "|86246|  205|  99|    9909| 104538848|15343|2012-03-02|       16.0|             OZ|               1|          2.49|\n",
      "|86246|  205|  59|    5907| 102900020| 2012|2012-03-02|       16.0|             OZ|               1|          1.39|\n",
      "|86246|  205|   9|     921| 101128414| 9209|2012-03-02|        4.0|             OZ|               2|           1.5|\n",
      "|86246|  205|  73|    7344|1068142161|20285|2012-03-02|        8.0|             CT|               1|          5.79|\n",
      "|86246|  205|  41|    4107| 104113040|28204|2012-03-02|       14.5|             OZ|               1|          0.59|\n",
      "|86246|  205|  21|    2106| 105100050|27873|2012-03-02|       64.0|             OZ|               1|          3.29|\n",
      "|86246|  205|   8|     814| 102840020|18584|2012-03-02|       15.5|             OZ|               1|          3.29|\n",
      "|86246|  205|  91|    9122| 108200080| 2911|2012-03-02|       10.0|             OZ|               1|          1.99|\n",
      "|86246|  205|  41|    4120| 101116616|15266|2012-03-02|        6.0|             OZ|               1|          0.89|\n",
      "|86246|  205|  63|    6315| 107996777|31373|2012-03-02|       64.0|             OZ|               1|          3.59|\n",
      "|86246|  205|   9|     907| 101410010|13791|2012-03-02|       24.0|             OZ|               1|          3.99|\n",
      "|86246|  205|  97|    9753|1021013323|    0|2012-03-02|        1.0|             CT|               1|          8.87|\n",
      "|86246|  205|  45|    4509|1082650484|59628|2012-03-02|       16.0|             OZ|               1|          4.99|\n",
      "|86246|  205|  26|    2630| 103700030|14647|2012-03-02|       56.0|             CT|               1|           1.0|\n",
      "+-----+-----+----+--------+----------+-----+----------+-----------+---------------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get number of refund transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------+\n",
      "|      id|total_trans|total_returns|\n",
      "+--------+-----------+-------------+\n",
      "|18470775|        350|            8|\n",
      "|14723452|        755|            5|\n",
      "|15738658|         39|            0|\n",
      "|17552659|        591|            4|\n",
      "|12996040|        326|            5|\n",
      "|16078766|        966|           54|\n",
      "|18249735|       1557|           60|\n",
      "|14989775|        614|           39|\n",
      "|15073302|        526|           38|\n",
      "|16075389|        591|           32|\n",
      "|16606739|        678|            6|\n",
      "|15705695|        431|            8|\n",
      "|14576147|        817|           66|\n",
      "|15134033|        944|           13|\n",
      "|16551772|       1699|           47|\n",
      "|17652157|       1407|           15|\n",
      "|13089312|       1218|           52|\n",
      "|13744500|       2232|          112|\n",
      "|16829614|        738|           10|\n",
      "|17524817|        328|            7|\n",
      "+--------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "returns = transactions.select('id',\n",
    "                              F.when(transactions['purchaseamount'] < 0, 1).otherwise(0).alias('return'))\\\n",
    "                      .withColumn('1', F.lit(1))\n",
    "returns = returns.groupBy('id').agg(F.sum('1').alias('total_trans'), F.sum(\"return\").alias('total_returns'))\n",
    "returns.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/trainHistory.csv')\n",
    "offers = pd.read_csv('../data/offers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "offers_dict = {}\n",
    "for e, line in enumerate(open('../data/offers.csv')):\n",
    "    row = line.strip().split(\",\")\n",
    "    offers_dict[row[0]] = row\n",
    "\n",
    "#keep two dictionaries with the shopper id's from test and train\n",
    "train_ids = {}\n",
    "test_ids = {}\n",
    "for e, line in enumerate(open('../data/trainHistory.csv')):\n",
    "    if e > 0:\n",
    "        row = line.strip().split(\",\")\n",
    "        train_ids[row[0]] = row\n",
    "for e, line in enumerate(open('../data/testHistory.csv')):\n",
    "    if e > 0:\n",
    "        row = line.strip().split(\",\")\n",
    "        test_ids[row[0]] = row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offer</th>\n",
       "      <th>category</th>\n",
       "      <th>quantity</th>\n",
       "      <th>company</th>\n",
       "      <th>offervalue</th>\n",
       "      <th>brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1190530</td>\n",
       "      <td>9115</td>\n",
       "      <td>1</td>\n",
       "      <td>108500080</td>\n",
       "      <td>5.00</td>\n",
       "      <td>93904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1194044</td>\n",
       "      <td>9909</td>\n",
       "      <td>1</td>\n",
       "      <td>107127979</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1197502</td>\n",
       "      <td>3203</td>\n",
       "      <td>1</td>\n",
       "      <td>106414464</td>\n",
       "      <td>0.75</td>\n",
       "      <td>13474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1198271</td>\n",
       "      <td>5558</td>\n",
       "      <td>1</td>\n",
       "      <td>107120272</td>\n",
       "      <td>1.50</td>\n",
       "      <td>5072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1198272</td>\n",
       "      <td>5558</td>\n",
       "      <td>1</td>\n",
       "      <td>107120272</td>\n",
       "      <td>1.50</td>\n",
       "      <td>5072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     offer  category  quantity    company  offervalue  brand\n",
       "0  1190530      9115         1  108500080        5.00  93904\n",
       "1  1194044      9909         1  107127979        1.00   6732\n",
       "2  1197502      3203         1  106414464        0.75  13474\n",
       "3  1198271      5558         1  107120272        1.50   5072\n",
       "4  1198272      5558         1  107120272        1.50   5072"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add department to offers\n",
    "dept = []\n",
    "for i in range(offers.shape[0]):\n",
    "    str_category = str(offers['category'].iloc[i])\n",
    "    if len(str_category) == 4:\n",
    "        dept.append(str_category[:2])\n",
    "    else:\n",
    "        dept.append(str_category[:1])\n",
    "        \n",
    "offers['dept'] = dept\n",
    "offers['dept'] = pd.to_numeric(offers['dept'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.merge(train, offers, how='left', on=['offer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "train.columns = ['id', 'chain', 'offer', 'market', 'repeattrips', 'repeater', 'offerdate', \n",
    "                 'offer_category', 'quantity', 'offer_company', 'offervalue', 'offer_brand',\n",
    "                 'offer_dept']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reencode the target variable\n",
    "train['repeater'] = np.where(train['repeater'] == 't', '1', '0')\n",
    "train['repeater'] = pd.to_numeric(train['repeater'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['offerdate'] = train['offerdate'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/testHistory.csv')\n",
    "test = pd.merge(test, offers, how='left', on=['offer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "test.columns = ['id', 'chain', 'offer', 'market', 'offerdate', 'offer_category', 'quantity', \n",
    "                'offer_company', 'offervalue', 'offer_brand', 'offer_dept']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['offerdate'] = test['offerdate'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_schema = StructType([\n",
    "    StructField(\"id\", LongType(), False),\n",
    "    StructField(\"offer_chain\", LongType(), True),\n",
    "    StructField(\"offer\", LongType(), True),\n",
    "    StructField(\"market\", LongType(), True),\n",
    "    StructField(\"repeattrips\", LongType(), True),\n",
    "    StructField(\"repeater\", IntegerType(), True),\n",
    "    StructField(\"offerdate\", StringType(), True),\n",
    "    StructField(\"offer_category\", LongType(), True),\n",
    "    StructField(\"quantity\", LongType(), True),\n",
    "    StructField(\"offer_company\", LongType(), True),\n",
    "    StructField(\"offervalue\", DoubleType(), True),\n",
    "    StructField(\"offer_brand\", LongType(), True),\n",
    "    StructField(\"offer_dept\", LongType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = sqlContext.createDataFrame(train, train_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_schema = StructType([\n",
    "    StructField(\"id\", LongType(), False),\n",
    "    StructField(\"offer_chain\", LongType(), True),\n",
    "    StructField(\"offer\", LongType(), True),\n",
    "    StructField(\"market\", LongType(), True),\n",
    "    StructField(\"offerdate\", StringType(), True),\n",
    "    StructField(\"offer_category\", LongType(), True),\n",
    "    StructField(\"quantity\", LongType(), True),\n",
    "    StructField(\"offer_company\", LongType(), True),\n",
    "    StructField(\"offervalue\", DoubleType(), True),\n",
    "    StructField(\"offer_brand\", LongType(), True),\n",
    "    StructField(\"offer_dept\", LongType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = sqlContext.createDataFrame(test, test_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function converts the string cell into a date:\n",
    "stringToDate = F.udf(lambda x: datetime.strptime(x, '%Y-%m-%d'), DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert offerdate to date type\n",
    "train_df = train_df.withColumn('offerdate', stringToDate(F.col('offerdate')))\n",
    "test_df = test_df.withColumn('offerdate', stringToDate(F.col('offerdate')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "offer_df = train_df.select('offer').union(test_df.select('offer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(offer=1208251),\n",
       " Row(offer=1197502),\n",
       " Row(offer=1197502),\n",
       " Row(offer=1197502),\n",
       " Row(offer=1204821)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offer_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def days_diff(d1,d2, date_format = \"%Y-%m-%d\"):\n",
    "    return (datetime.strptime(d2, date_format) - datetime.strptime(d1, date_format)).days\n",
    "\n",
    "def dow(d1, date_format = \"%Y-%m-%d\"):\n",
    "    return datetime.strptime(d1, date_format).weekday()\n",
    "\n",
    "def dom(d1, date_format = \"%Y-%m-%d\"):\n",
    "    return datetime.strptime(d1, date_format).day\n",
    "\n",
    "def month(d1, date_format = \"%Y-%m-%d\"):\n",
    "    return datetime.strptime(d1, date_format).month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print month(\"2013-04-24\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=86246, offer_chain=205, offer=1208251, market=34, repeattrips=5, repeater=1, offerdate=datetime.date(2013, 4, 24), offer_category=2202, quantity=1, offer_company=104460040, offervalue=2.0, offer_brand=3718, offer_dept=22),\n",
       " Row(id=86252, offer_chain=205, offer=1197502, market=34, repeattrips=16, repeater=1, offerdate=datetime.date(2013, 3, 27), offer_category=3203, quantity=1, offer_company=106414464, offervalue=0.75, offer_brand=13474, offer_dept=32),\n",
       " Row(id=12682470, offer_chain=18, offer=1197502, market=11, repeattrips=0, repeater=0, offerdate=datetime.date(2013, 3, 28), offer_category=3203, quantity=1, offer_company=106414464, offervalue=0.75, offer_brand=13474, offer_dept=32),\n",
       " Row(id=12996040, offer_chain=15, offer=1197502, market=9, repeattrips=0, repeater=0, offerdate=datetime.date(2013, 3, 25), offer_category=3203, quantity=1, offer_company=106414464, offervalue=0.75, offer_brand=13474, offer_dept=32),\n",
       " Row(id=13089312, offer_chain=15, offer=1204821, market=9, repeattrips=0, repeater=0, offerdate=datetime.date(2013, 4, 1), offer_category=5619, quantity=1, offer_company=107717272, offervalue=1.5, offer_brand=102504, offer_dept=56)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### THE FOLLOWING CODE FOR BUILDING THE FEATURES IS DERIVED FROM THIS KAGGLE FORUM POST: https://www.kaggle.com/c/acquire-valued-shoppers-challenge/forums/t/7688/feature-engineering-and-beat-the-benchmark-0-59347\n",
    "\n",
    "loc_offers = \"../data/offers.csv\"\n",
    "loc_transactions = \"../data/transactions.csv\"\n",
    "loc_train = \"../data/trainHistory.csv\"\n",
    "loc_test = \"../data/testHistory.csv\"\n",
    "\n",
    "# will be created\n",
    "loc_reduced = \"../data/reduced.csv\" \n",
    "loc_out_train = \"../data/train.csv\"\n",
    "loc_out_test = \"../data/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce_data(loc_offers, loc_transactions, loc_reduced):\n",
    "  start = datetime.now()\n",
    "  #get all categories and comps on offer in a dict\n",
    "  offers_cat = {}\n",
    "  offers_co = {}\n",
    "  for e, line in enumerate( open(loc_offers) ):\n",
    "    offers_cat[ line.split(\",\")[1] ] = 1\n",
    "    offers_co[ line.split(\",\")[3] ] = 1\n",
    "  #open output file\n",
    "  with open(loc_reduced, \"wb\") as outfile:\n",
    "    #go through transactions file and reduce\n",
    "    reduced = 0\n",
    "    for e, line in enumerate( open(loc_transactions) ):\n",
    "      if e == 0:\n",
    "        outfile.write( line ) #print header\n",
    "      else:\n",
    "        #only write when if category in offers dict\n",
    "        if line.split(\",\")[3] in offers_cat or line.split(\",\")[4] in offers_co:\n",
    "          outfile.write( line )\n",
    "          reduced += 1\n",
    "      #progress\n",
    "      if e % 5000000 == 0:\n",
    "        print e, reduced, datetime.now() - start\n",
    "  print e, reduced, datetime.now() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reduce_data(loc_offers, loc_transactions, loc_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_features(loc_train, loc_test, loc_transactions, loc_out_train, loc_out_test):\n",
    "    with open(loc_out_train, \"wb\") as out_train, open(loc_out_test, \"wb\") as out_test:\n",
    "        #iterate through reduced dataset \n",
    "        last_id = 0\n",
    "        features = defaultdict(float)\n",
    "        for e, line in enumerate(open(loc_transactions)):\n",
    "            if e > 0: #skip header\n",
    "                #poor man's csv reader\n",
    "                row = line.strip().split(\",\")\n",
    "                #write away the features when we get to a new shopper id\n",
    "                if last_id != row[0] and e != 1:\n",
    "\n",
    "                    #generate negative features\n",
    "                    if \"has_bought_company\" not in features:\n",
    "                        features['never_bought_company'] = 1\n",
    "\n",
    "                    if \"has_bought_category\" not in features:\n",
    "                        features['never_bought_category'] = 1\n",
    "\n",
    "                    if \"has_bought_brand\" not in features:\n",
    "                        features['never_bought_brand'] = 1\n",
    "\n",
    "                    if \"has_bought_brand\" in features and \"has_bought_category\" in features and \"has_bought_company\" in features:\n",
    "                        features['has_bought_brand_company_category'] = 1\n",
    "\n",
    "                    if \"has_bought_brand\" in features and \"has_bought_category\" in features:\n",
    "                        features['has_bought_brand_category'] = 1\n",
    "\n",
    "                    if \"has_bought_brand\" in features and \"has_bought_company\" in features:\n",
    "                        features['has_bought_brand_company'] = 1\n",
    "\n",
    "                    test = False\n",
    "                    for k, v in features.items():\n",
    "\n",
    "                        if k == \"label\" and v == 0.5:\n",
    "                            #test\n",
    "                            test = True\n",
    "                    \n",
    "                    if test:\n",
    "                        features[\"id\"] = last_id\n",
    "                        out_test.write(str(features))\n",
    "                    else:\n",
    "                        features[\"id\"] = last_id\n",
    "                        out_train.write(str(features))\n",
    "                    #print \"Writing features or storing them in an array\"\n",
    "                    #reset features\n",
    "                    features = defaultdict(float)\n",
    "                #generate features from transaction record\n",
    "                #check if we have a test sample or train sample\n",
    "                if row[0] in train_ids or row[0] in test_ids:\n",
    "                    #generate label and history\n",
    "                    if row[0] in train_ids:\n",
    "                        history = train_ids[row[0]]\n",
    "                        if train_ids[row[0]][5] == \"t\":\n",
    "                            features['label'] = 1\n",
    "                        else:\n",
    "                            features['label'] = 0\n",
    "                    else:\n",
    "                        history = test_ids[row[0]]\n",
    "                        features['label'] = 0.5\n",
    "\n",
    "                    #print \"label\", label \n",
    "                    #print \"trainhistory\", train_ids[row[0]]\n",
    "                    #print \"transaction\", row\n",
    "                    #print \"offers\", offers_dict[ train_ids[row[0]][2] ]\n",
    "                    #print\n",
    "\n",
    "                    features['offer_value'] = offers_dict[ history[2] ][4]\n",
    "                    features['offer_quantity'] = offers_dict[ history[2] ][2]\n",
    "                    offervalue = offers_dict[ history[2] ][4]\n",
    "\n",
    "                    features['total_spend'] += float( row[10] )\n",
    "                    \n",
    "                    features['day_of_week'] = dow(history[-1])\n",
    "                    features['day_of_month'] = dom(history[-1])\n",
    "                    features['month'] = month(history[-1])\n",
    "\n",
    "                    if offers_dict[ history[2] ][3] == row[4]:\n",
    "                        features['has_bought_company'] += 1.0\n",
    "                        features['has_bought_company_q'] += float( row[9] )\n",
    "                        features['has_bought_company_a'] += float( row[10] )\n",
    "                        days_diff\n",
    "                        date_diff_days = days_diff(row[6],history[-1])\n",
    "                        if date_diff_days < 30:\n",
    "                            features['has_bought_company_30'] += 1.0\n",
    "                            features['has_bought_company_q_30'] += float( row[9] )\n",
    "                            features['has_bought_company_a_30'] += float( row[10] )\n",
    "                        if date_diff_days < 60:\n",
    "                            features['has_bought_company_60'] += 1.0\n",
    "                            features['has_bought_company_q_60'] += float( row[9] )\n",
    "                            features['has_bought_company_a_60'] += float( row[10] )\n",
    "                        if date_diff_days < 90:\n",
    "                            features['has_bought_company_90'] += 1.0\n",
    "                            features['has_bought_company_q_90'] += float( row[9] )\n",
    "                            features['has_bought_company_a_90'] += float( row[10] )\n",
    "                        if date_diff_days < 180:\n",
    "                            features['has_bought_company_180'] += 1.0\n",
    "                            features['has_bought_company_q_180'] += float( row[9] )\n",
    "                            features['has_bought_company_a_180'] += float( row[10] )\n",
    "\n",
    "                    if offers_dict[ history[2] ][1] == row[3]:\n",
    "\n",
    "                        features['has_bought_category'] += 1.0\n",
    "                        features['has_bought_category_q'] += float( row[9] )\n",
    "                        features['has_bought_category_a'] += float( row[10] )\n",
    "                        date_diff_days = days_diff(row[6],history[-1])\n",
    "                        if date_diff_days < 30:\n",
    "                            features['has_bought_category_30'] += 1.0\n",
    "                            features['has_bought_category_q_30'] += float( row[9] )\n",
    "                            features['has_bought_category_a_30'] += float( row[10] )\n",
    "                        if date_diff_days < 60:\n",
    "                            features['has_bought_category_60'] += 1.0\n",
    "                            features['has_bought_category_q_60'] += float( row[9] )\n",
    "                            features['has_bought_category_a_60'] += float( row[10] )\n",
    "                        if date_diff_days < 90:\n",
    "                            features['has_bought_category_90'] += 1.0\n",
    "                            features['has_bought_category_q_90'] += float( row[9] )\n",
    "                            features['has_bought_category_a_90'] += float( row[10] )\t\t\t\t\t\t\n",
    "                        if date_diff_days < 180:\n",
    "                            features['has_bought_category_180'] += 1.0\n",
    "                            features['has_bought_category_q_180'] += float( row[9] )\n",
    "                            features['has_bought_category_a_180'] += float( row[10] )\t\t\t\t\n",
    "                    if offers_dict[ history[2] ][5] == row[5]:\n",
    "                        features['has_bought_brand'] += 1.0\n",
    "                        features['has_bought_brand_q'] += float( row[9] )\n",
    "                        features['has_bought_brand_a'] += float( row[10] )\n",
    "                        date_diff_days = days_diff(row[6],history[-1])\n",
    "                        if date_diff_days < 30:\n",
    "                            features['has_bought_brand_30'] += 1.0\n",
    "                            features['has_bought_brand_q_30'] += float( row[9] )\n",
    "                            features['has_bought_brand_a_30'] += float( row[10] )\n",
    "                        if date_diff_days < 60:\n",
    "                            features['has_bought_brand_60'] += 1.0\n",
    "                            features['has_bought_brand_q_60'] += float( row[9] )\n",
    "                            features['has_bought_brand_a_60'] += float( row[10] )\n",
    "                        if date_diff_days < 90:\n",
    "                            features['has_bought_brand_90'] += 1.0\n",
    "                            features['has_bought_brand_q_90'] += float( row[9] )\n",
    "                            features['has_bought_brand_a_90'] += float( row[10] )\t\t\t\t\t\t\n",
    "                        if date_diff_days < 180:\n",
    "                            features['has_bought_brand_180'] += 1.0\n",
    "                            features['has_bought_brand_q_180'] += float( row[9] )\n",
    "                            features['has_bought_brand_a_180'] += float( row[10] )\t\n",
    "                last_id = row[0]\n",
    "                if e % 100000 == 0:\n",
    "                    print e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(columns=('id', 'label', 'offer_value', 'offer_quantity', 'total_spend', 'day_of_week', 'day_of_month', 'month', 'has_bought_company', 'has_bought_company_q', 'has_bought_company_a', 'has_bought_company_30', 'has_bought_company_q_30', 'has_bought_company_a_30', 'has_bought_company_60', 'has_bought_company_q_60', 'has_bought_company_a_60', 'has_bought_company_90', 'has_bought_company_q_90', 'has_bought_company_a_90', 'has_bought_company_180', 'has_bought_company_q_180', 'has_bought_company_a_180', 'has_bought_category', 'has_bought_category_q', 'has_bought_category_a', 'has_bought_category_30', 'has_bought_category_q_30', 'has_bought_category_a_30', 'has_bought_category_60', 'has_bought_category_q_60', 'has_bought_category_a_60', 'has_bought_category_90', 'has_bought_category_q_90', 'has_bought_category_a_90', 'has_bought_category_180', 'has_bought_category_q_180', 'has_bought_category_a_180', 'has_bought_brand', 'has_bought_brand_q', 'has_bought_brand_a', 'has_bought_brand_30', 'has_bought_brand_q_30', 'has_bought_brand_a_30', 'has_bought_brand_60', 'has_bought_brand_q_60', 'has_bought_brand_a_60', 'has_bought_brand_90', 'has_bought_brand_q_90', 'has_bought_brand_a_90', \n",
    "'has_bought_brand_180', 'has_bought_brand_q_180', 'has_bought_brand_a_180', 'never_bought_company', 'never_bought_category', 'never_bought_brand', 'has_bought_brand_company_category', 'has_bought_brand_category', 'has_bought_brand_company'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(columns=('id', 'label', 'offer_value', 'offer_quantity', 'total_spend', 'day_of_week', 'day_of_month', 'month', 'has_bought_company', 'has_bought_company_q', 'has_bought_company_a', 'has_bought_company_30', 'has_bought_company_q_30', 'has_bought_company_a_30', 'has_bought_company_60', 'has_bought_company_q_60', 'has_bought_company_a_60', 'has_bought_company_90', 'has_bought_company_q_90', 'has_bought_company_a_90', 'has_bought_company_180', 'has_bought_company_q_180', 'has_bought_company_a_180', 'has_bought_category', 'has_bought_category_q', 'has_bought_category_a', 'has_bought_category_30', 'has_bought_category_q_30', 'has_bought_category_a_30', 'has_bought_category_60', 'has_bought_category_q_60', 'has_bought_category_a_60', 'has_bought_category_90', 'has_bought_category_q_90', 'has_bought_category_a_90', 'has_bought_category_180', 'has_bought_category_q_180', 'has_bought_category_a_180', 'has_bought_brand', 'has_bought_brand_q', 'has_bought_brand_a', 'has_bought_brand_30', 'has_bought_brand_q_30', 'has_bought_brand_a_30', 'has_bought_brand_60', 'has_bought_brand_q_60', 'has_bought_brand_a_60', 'has_bought_brand_90', 'has_bought_brand_q_90', 'has_bought_brand_a_90', \n",
    "'has_bought_brand_180', 'has_bought_brand_q_180', 'has_bought_brand_a_180', 'never_bought_company', 'never_bought_category', 'never_bought_brand', 'has_bought_brand_company_category', 'has_bought_brand_category', 'has_bought_brand_company'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 59)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'id', u'label', u'offer_value', u'offer_quantity', u'total_spend',\n",
       "       u'day_of_week', u'day_of_month', u'month', u'has_bought_company',\n",
       "       u'has_bought_company_q', u'has_bought_company_a',\n",
       "       u'has_bought_company_30', u'has_bought_company_q_30',\n",
       "       u'has_bought_company_a_30', u'has_bought_company_60',\n",
       "       u'has_bought_company_q_60', u'has_bought_company_a_60',\n",
       "       u'has_bought_company_90', u'has_bought_company_q_90',\n",
       "       u'has_bought_company_a_90', u'has_bought_company_180',\n",
       "       u'has_bought_company_q_180', u'has_bought_company_a_180',\n",
       "       u'has_bought_category', u'has_bought_category_q',\n",
       "       u'has_bought_category_a', u'has_bought_category_30',\n",
       "       u'has_bought_category_q_30', u'has_bought_category_a_30',\n",
       "       u'has_bought_category_60', u'has_bought_category_q_60',\n",
       "       u'has_bought_category_a_60', u'has_bought_category_90',\n",
       "       u'has_bought_category_q_90', u'has_bought_category_a_90',\n",
       "       u'has_bought_category_180', u'has_bought_category_q_180',\n",
       "       u'has_bought_category_a_180', u'has_bought_brand',\n",
       "       u'has_bought_brand_q', u'has_bought_brand_a', u'has_bought_brand_30',\n",
       "       u'has_bought_brand_q_30', u'has_bought_brand_a_30',\n",
       "       u'has_bought_brand_60', u'has_bought_brand_q_60',\n",
       "       u'has_bought_brand_a_60', u'has_bought_brand_90',\n",
       "       u'has_bought_brand_q_90', u'has_bought_brand_a_90',\n",
       "       u'has_bought_brand_180', u'has_bought_brand_q_180',\n",
       "       u'has_bought_brand_a_180', u'never_bought_company',\n",
       "       u'never_bought_category', u'never_bought_brand',\n",
       "       u'has_bought_brand_company_category', u'has_bought_brand_category',\n",
       "       u'has_bought_brand_company'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [safefetch(features, column) for column in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def safefetch(features, column):\n",
    "    if column in features:\n",
    "        return features[column]\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_list = []\n",
    "test_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_features(loc_train, loc_test, loc_transactions, loc_out_train, loc_out_test, train_list = train_list, test_list = test_list):\n",
    "#     with open(loc_out_train, \"wb\") as out_train, open(loc_out_test, \"wb\") as out_test:\n",
    "        #iterate through reduced dataset\n",
    "        columns = ['id', 'label', 'offer_value', 'offer_quantity', 'total_spend', 'day_of_week', 'day_of_month', 'month', 'has_bought_company', 'has_bought_company_q', 'has_bought_company_a', 'has_bought_company_30', 'has_bought_company_q_30', 'has_bought_company_a_30', 'has_bought_company_60', 'has_bought_company_q_60', 'has_bought_company_a_60', 'has_bought_company_90', 'has_bought_company_q_90', 'has_bought_company_a_90', 'has_bought_company_180', 'has_bought_company_q_180', 'has_bought_company_a_180', 'has_bought_category', 'has_bought_category_q', 'has_bought_category_a', 'has_bought_category_30', 'has_bought_category_q_30', 'has_bought_category_a_30', 'has_bought_category_60', 'has_bought_category_q_60', 'has_bought_category_a_60', 'has_bought_category_90', 'has_bought_category_q_90', 'has_bought_category_a_90', 'has_bought_category_180', 'has_bought_category_q_180', 'has_bought_category_a_180', 'has_bought_brand', 'has_bought_brand_q', 'has_bought_brand_a', 'has_bought_brand_30', 'has_bought_brand_q_30', 'has_bought_brand_a_30', 'has_bought_brand_60', 'has_bought_brand_q_60', 'has_bought_brand_a_60', 'has_bought_brand_90', 'has_bought_brand_q_90', 'has_bought_brand_a_90', \n",
    "'has_bought_brand_180', 'has_bought_brand_q_180', 'has_bought_brand_a_180', 'never_bought_company', 'never_bought_category', 'never_bought_brand', 'has_bought_brand_company_category', 'has_bought_brand_category', 'has_bought_brand_company']\n",
    "        last_id = 0\n",
    "        features = defaultdict(float)\n",
    "        for e, line in enumerate(open(loc_transactions)):\n",
    "            if e > 0: #skip header\n",
    "                #poor man's csv reader\n",
    "                row = line.strip().split(\",\")\n",
    "                #write away the features when we get to a new shopper id\n",
    "                if last_id != row[0] and e != 1:\n",
    "\n",
    "                    #generate negative features\n",
    "#                     print train_df\n",
    "                    if \"has_bought_company\" not in features:\n",
    "                        features['never_bought_company'] = 1\n",
    "\n",
    "                    if \"has_bought_category\" not in features:\n",
    "                        features['never_bought_category'] = 1\n",
    "\n",
    "                    if \"has_bought_brand\" not in features:\n",
    "                        features['never_bought_brand'] = 1\n",
    "\n",
    "                    if \"has_bought_brand\" in features and \"has_bought_category\" in features and \"has_bought_company\" in features:\n",
    "                        features['has_bought_brand_company_category'] = 1\n",
    "\n",
    "                    if \"has_bought_brand\" in features and \"has_bought_category\" in features:\n",
    "                        features['has_bought_brand_category'] = 1\n",
    "\n",
    "                    if \"has_bought_brand\" in features and \"has_bought_company\" in features:\n",
    "                        features['has_bought_brand_company'] = 1\n",
    "\n",
    "                    test = False\n",
    "                    for k, v in features.items():\n",
    "\n",
    "                        if k == \"label\" and v == 0.5:\n",
    "                            #test\n",
    "                            test = True\n",
    "                    \n",
    "                    if test:\n",
    "                        features[\"id\"] = last_id\n",
    "                        test_list.append([safefetch(features, column) for column in columns])\n",
    "                    else:\n",
    "                        features[\"id\"] = last_id\n",
    "                        train_list.append([safefetch(features, column) for column in columns])\n",
    "                    #print \"Writing features or storing them in an array\"\n",
    "                    #reset features\n",
    "                    features = defaultdict(float)\n",
    "                #generate features from transaction record\n",
    "                #check if we have a test sample or train sample\n",
    "                if row[0] in train_ids or row[0] in test_ids:\n",
    "                    #generate label and history\n",
    "                    if row[0] in train_ids:\n",
    "                        history = train_ids[row[0]]\n",
    "                        if train_ids[row[0]][5] == \"t\":\n",
    "                            features['label'] = 1\n",
    "                        else:\n",
    "                            features['label'] = 0\n",
    "                    else:\n",
    "                        history = test_ids[row[0]]\n",
    "                        features['label'] = 0.5\n",
    "\n",
    "                    #print \"label\", label \n",
    "                    #print \"trainhistory\", train_ids[row[0]]\n",
    "                    #print \"transaction\", row\n",
    "                    #print \"offers\", offers_dict[ train_ids[row[0]][2] ]\n",
    "                    #print\n",
    "\n",
    "                    features['offer_value'] = offers_dict[ history[2] ][4]\n",
    "                    features['offer_quantity'] = offers_dict[ history[2] ][2]\n",
    "                    offervalue = offers_dict[ history[2] ][4]\n",
    "\n",
    "                    features['total_spend'] += float( row[10] )\n",
    "                    \n",
    "                    features['day_of_week'] = dow(history[-1])\n",
    "                    features['day_of_month'] = dom(history[-1])\n",
    "                    features['month'] = month(history[-1])\n",
    "\n",
    "                    if offers_dict[ history[2] ][3] == row[4]:\n",
    "                        features['has_bought_company'] += 1.0\n",
    "                        features['has_bought_company_q'] += float( row[9] )\n",
    "                        features['has_bought_company_a'] += float( row[10] )\n",
    "                        days_diff\n",
    "                        date_diff_days = days_diff(row[6],history[-1])\n",
    "                        if date_diff_days < 30:\n",
    "                            features['has_bought_company_30'] += 1.0\n",
    "                            features['has_bought_company_q_30'] += float( row[9] )\n",
    "                            features['has_bought_company_a_30'] += float( row[10] )\n",
    "                        if date_diff_days < 60:\n",
    "                            features['has_bought_company_60'] += 1.0\n",
    "                            features['has_bought_company_q_60'] += float( row[9] )\n",
    "                            features['has_bought_company_a_60'] += float( row[10] )\n",
    "                        if date_diff_days < 90:\n",
    "                            features['has_bought_company_90'] += 1.0\n",
    "                            features['has_bought_company_q_90'] += float( row[9] )\n",
    "                            features['has_bought_company_a_90'] += float( row[10] )\n",
    "                        if date_diff_days < 180:\n",
    "                            features['has_bought_company_180'] += 1.0\n",
    "                            features['has_bought_company_q_180'] += float( row[9] )\n",
    "                            features['has_bought_company_a_180'] += float( row[10] )\n",
    "\n",
    "                    if offers_dict[ history[2] ][1] == row[3]:\n",
    "\n",
    "                        features['has_bought_category'] += 1.0\n",
    "                        features['has_bought_category_q'] += float( row[9] )\n",
    "                        features['has_bought_category_a'] += float( row[10] )\n",
    "                        date_diff_days = days_diff(row[6],history[-1])\n",
    "                        if date_diff_days < 30:\n",
    "                            features['has_bought_category_30'] += 1.0\n",
    "                            features['has_bought_category_q_30'] += float( row[9] )\n",
    "                            features['has_bought_category_a_30'] += float( row[10] )\n",
    "                        if date_diff_days < 60:\n",
    "                            features['has_bought_category_60'] += 1.0\n",
    "                            features['has_bought_category_q_60'] += float( row[9] )\n",
    "                            features['has_bought_category_a_60'] += float( row[10] )\n",
    "                        if date_diff_days < 90:\n",
    "                            features['has_bought_category_90'] += 1.0\n",
    "                            features['has_bought_category_q_90'] += float( row[9] )\n",
    "                            features['has_bought_category_a_90'] += float( row[10] )\t\t\t\t\t\t\n",
    "                        if date_diff_days < 180:\n",
    "                            features['has_bought_category_180'] += 1.0\n",
    "                            features['has_bought_category_q_180'] += float( row[9] )\n",
    "                            features['has_bought_category_a_180'] += float( row[10] )\t\t\t\t\n",
    "                    if offers_dict[ history[2] ][5] == row[5]:\n",
    "                        features['has_bought_brand'] += 1.0\n",
    "                        features['has_bought_brand_q'] += float( row[9] )\n",
    "                        features['has_bought_brand_a'] += float( row[10] )\n",
    "                        date_diff_days = days_diff(row[6],history[-1])\n",
    "                        if date_diff_days < 30:\n",
    "                            features['has_bought_brand_30'] += 1.0\n",
    "                            features['has_bought_brand_q_30'] += float( row[9] )\n",
    "                            features['has_bought_brand_a_30'] += float( row[10] )\n",
    "                        if date_diff_days < 60:\n",
    "                            features['has_bought_brand_60'] += 1.0\n",
    "                            features['has_bought_brand_q_60'] += float( row[9] )\n",
    "                            features['has_bought_brand_a_60'] += float( row[10] )\n",
    "                        if date_diff_days < 90:\n",
    "                            features['has_bought_brand_90'] += 1.0\n",
    "                            features['has_bought_brand_q_90'] += float( row[9] )\n",
    "                            features['has_bought_brand_a_90'] += float( row[10] )\t\t\t\t\t\t\n",
    "                        if date_diff_days < 180:\n",
    "                            features['has_bought_brand_180'] += 1.0\n",
    "                            features['has_bought_brand_q_180'] += float( row[9] )\n",
    "                            features['has_bought_brand_a_180'] += float( row[10] )\t\n",
    "                last_id = row[0]\n",
    "                if e % 100000 == 0:\n",
    "                    print e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = ['id', 'label', 'offer_value', 'offer_quantity', 'total_spend', 'day_of_week', 'day_of_month', 'month', 'has_bought_company', 'has_bought_company_q', 'has_bought_company_a', 'has_bought_company_30', 'has_bought_company_q_30', 'has_bought_company_a_30', 'has_bought_company_60', 'has_bought_company_q_60', 'has_bought_company_a_60', 'has_bought_company_90', 'has_bought_company_q_90', 'has_bought_company_a_90', 'has_bought_company_180', 'has_bought_company_q_180', 'has_bought_company_a_180', 'has_bought_category', 'has_bought_category_q', 'has_bought_category_a', 'has_bought_category_30', 'has_bought_category_q_30', 'has_bought_category_a_30', 'has_bought_category_60', 'has_bought_category_q_60', 'has_bought_category_a_60', 'has_bought_category_90', 'has_bought_category_q_90', 'has_bought_category_a_90', 'has_bought_category_180', 'has_bought_category_q_180', 'has_bought_category_a_180', 'has_bought_brand', 'has_bought_brand_q', 'has_bought_brand_a', 'has_bought_brand_30', 'has_bought_brand_q_30', 'has_bought_brand_a_30', 'has_bought_brand_60', 'has_bought_brand_q_60', 'has_bought_brand_a_60', 'has_bought_brand_90', 'has_bought_brand_q_90', 'has_bought_brand_a_90', \n",
    "'has_bought_brand_180', 'has_bought_brand_q_180', 'has_bought_brand_a_180', 'never_bought_company', 'never_bought_category', 'never_bought_brand', 'has_bought_brand_company_category', 'has_bought_brand_category', 'has_bought_brand_company']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n"
     ]
    }
   ],
   "source": [
    "generate_features(loc_train, loc_test, \"../data/reduced.csv\", loc_out_train, loc_out_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = ['id', 'label', 'offer_value', 'offer_quantity', 'total_spend', 'day_of_week', 'day_of_month', 'month', 'has_bought_company', 'has_bought_company_q', 'has_bought_company_a', 'has_bought_company_30', 'has_bought_company_q_30', 'has_bought_company_a_30', 'has_bought_company_60', 'has_bought_company_q_60', 'has_bought_company_a_60', 'has_bought_company_90', 'has_bought_company_q_90', 'has_bought_company_a_90', 'has_bought_company_180', 'has_bought_company_q_180', 'has_bought_company_a_180', 'has_bought_category', 'has_bought_category_q', 'has_bought_category_a', 'has_bought_category_30', 'has_bought_category_q_30', 'has_bought_category_a_30', 'has_bought_category_60', 'has_bought_category_q_60', 'has_bought_category_a_60', 'has_bought_category_90', 'has_bought_category_q_90', 'has_bought_category_a_90', 'has_bought_category_180', 'has_bought_category_q_180', 'has_bought_category_a_180', 'has_bought_brand', 'has_bought_brand_q', 'has_bought_brand_a', 'has_bought_brand_30', 'has_bought_brand_q_30', 'has_bought_brand_a_30', 'has_bought_brand_60', 'has_bought_brand_q_60', 'has_bought_brand_a_60', 'has_bought_brand_90', 'has_bought_brand_q_90', 'has_bought_brand_a_90', \n",
    "'has_bought_brand_180', 'has_bought_brand_q_180', 'has_bought_brand_a_180', 'never_bought_company', 'never_bought_category', 'never_bought_brand', 'has_bought_brand_company_category', 'has_bought_brand_category', 'has_bought_brand_company']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_list,columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(test_list,columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df.to_csv(loc_out_train, index = False)\n",
    "test_df.to_csv(loc_out_test, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example = example.append(['12262064', 0.5, '1.5', '1', 4118.179999999954, 3, 27, 6, 1.0, 1.0, 1.95, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3.0, 4.0, 56.79, 0, 0, 0, 1.0, 1.0, 2.79, 1.0, 1.0, 2.79, 1.0, 1.0, 2.79, 0, 1, 0, 0, 0, 1], axis =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df.to_csv(loc_out_train, index =False)\n",
    "test_df.to_csv(loc_out_test, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Convert offers to numerical categories\n",
    "indexer = StringIndexer().setInputCol(\"offer\").setOutputCol(\"offer_idx\").fit(offer_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "# One-hot-encoding of offer category\n",
    "encoder = OneHotEncoder().setOutputCol(\"encoded\").setDropLast(False)\n",
    "train_df = encoder.setInputCol(\"offer_idx\").transform(indexer.transform(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"encoded\"], outputCol=\"features\")\n",
    "train_df = train_df.withColumnRenamed('repeater', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = assembler.transform(train_df)\n",
    "output.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(maxIter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "lrModel = lr.fit(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_pred = lrModel.transform(output.select('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_pred.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting on Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df = encoder.setInputCol(\"offer_idx\").transform(indexer.transform(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_output = assembler.transform(test_df)\n",
    "test_output.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred = lrModel.transform(test_output.select('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_id = test_df.select('id').toPandas()\n",
    "test_prob = test_pred.select('probability').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prob = []\n",
    "for i in range(test_prob.shape[0]):\n",
    "    prob.append(test_prob['probability'][i][1])\n",
    "test_prob['probYes'] = prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_final_pred = pd.merge(test_id, test_prob, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>probability</th>\n",
       "      <th>probYes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12262064</td>\n",
       "      <td>[0.737425892072, 0.262574107928]</td>\n",
       "      <td>0.262574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12277270</td>\n",
       "      <td>[0.737425892072, 0.262574107928]</td>\n",
       "      <td>0.262574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12332190</td>\n",
       "      <td>[0.737425892072, 0.262574107928]</td>\n",
       "      <td>0.262574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12524696</td>\n",
       "      <td>[0.737425892072, 0.262574107928]</td>\n",
       "      <td>0.262574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13074629</td>\n",
       "      <td>[0.737425892072, 0.262574107928]</td>\n",
       "      <td>0.262574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                       probability   probYes\n",
       "0  12262064  [0.737425892072, 0.262574107928]  0.262574\n",
       "1  12277270  [0.737425892072, 0.262574107928]  0.262574\n",
       "2  12332190  [0.737425892072, 0.262574107928]  0.262574\n",
       "3  12524696  [0.737425892072, 0.262574107928]  0.262574\n",
       "4  13074629  [0.737425892072, 0.262574107928]  0.262574"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_final_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    151484.000000\n",
       "mean          0.262794\n",
       "std           0.018403\n",
       "min           0.143080\n",
       "25%           0.262574\n",
       "50%           0.262574\n",
       "75%           0.262574\n",
       "max           0.434382\n",
       "Name: probYes, dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_final_pred['probYes'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test(x):\n",
    "    if a == 3:\n",
    "        print \"yes\"\n",
    "    a +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'a' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-b00c77e39d62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-50-dd1af5273634>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"yes\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'a' referenced before assignment"
     ]
    }
   ],
   "source": [
    "test(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
