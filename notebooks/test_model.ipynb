{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_name = '../data/transactions_chunk1.csv'\n",
    "lines = sc.textFile(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "header = lines.first()\n",
    "lines = lines.filter(lambda row: row != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions for parsing the strings from the CSV file\n",
    "\n",
    "def toIntSafe(inval):\n",
    "  try:\n",
    "    return int(inval)\n",
    "  except ValueError:\n",
    "    return None\n",
    "\n",
    "def toTimeSafe(inval):\n",
    "  try:\n",
    "    return datetime.strptime(inval, \"%Y-%m-%d\")\n",
    "  except ValueError:\n",
    "    return None\n",
    "\n",
    "def toLongSafe(inval):\n",
    "  try:\n",
    "    return long(inval)\n",
    "  except ValueError:\n",
    "    return None\n",
    "\n",
    "def toFloatSafe(inval):\n",
    "  try:\n",
    "    return float(inval)\n",
    "  except ValueError:\n",
    "    return None\n",
    "    \n",
    "def stringToPost(row):\n",
    "  r = row.encode('utf-8').split(',')\n",
    "  return Row(\n",
    "    int(r[0]),         # Don't want this column to be nullable\n",
    "    toLongSafe(r[1]),\n",
    "    toLongSafe(r[2]),\n",
    "    toLongSafe(r[3]),\n",
    "    toLongSafe(r[4]),\n",
    "    toLongSafe(r[5]),\n",
    "    toTimeSafe(r[6]),\n",
    "    toFloatSafe(r[7]),\n",
    "    r[8],\n",
    "    toLongSafe(r[9]),\n",
    "    toFloatSafe(r[10]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transactions_schema = StructType([\n",
    "    StructField(\"id\", LongType(), False),\n",
    "    StructField(\"chain\", LongType(), True),\n",
    "    StructField(\"dept\", LongType(), True),\n",
    "    StructField(\"category\", LongType(), True),\n",
    "    StructField(\"company\", LongType(), True),\n",
    "    StructField(\"brand\", LongType(), True),\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"productsize\", DoubleType(), True),\n",
    "    StructField(\"purchasemeasure\", StringType(), True),\n",
    "    StructField(\"purchasequantity\", LongType(), True),\n",
    "    StructField(\"purchaseamount\", DoubleType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rowRDD = lines.map(lambda p: stringToPost(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transactions = sqlContext.createDataFrame(rowRDD, transactions_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/trainHistory.csv')\n",
    "offers = pd.read_csv('../data/offers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add department to offers\n",
    "dept = []\n",
    "for i in range(offers.shape[0]):\n",
    "    str_category = str(offers['category'].iloc[i])\n",
    "    if len(str_category) == 4:\n",
    "        dept.append(str_category[:2])\n",
    "    else:\n",
    "        dept.append(str_category[:1])\n",
    "        \n",
    "offers['dept'] = dept\n",
    "offers['dept'] = pd.to_numeric(offers['dept'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.merge(train, offers, how='left', on=['offer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "train.columns = ['id', 'chain', 'offer', 'market', 'repeattrips', 'repeater', 'offerdate', \n",
    "                 'offer_category', 'quantity', 'offer_company', 'offervalue', 'offer_brand',\n",
    "                 'offer_dept']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reencode the target variable\n",
    "train['repeater'] = np.where(train['repeater'] == 't', '1', '0')\n",
    "train['repeater'] = pd.to_numeric(train['repeater'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['offerdate'] = train['offerdate'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/testHistory.csv')\n",
    "test = pd.merge(test, offers, how='left', on=['offer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "test.columns = ['id', 'chain', 'offer', 'market', 'offerdate', 'offer_category', 'quantity', \n",
    "                'offer_company', 'offervalue', 'offer_brand', 'offer_dept']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['offerdate'] = test['offerdate'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add in Shiva's features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_more = pd.read_csv('../data/train.csv')\n",
    "train_more.drop(['label', 'offer_value', 'offer_quantity'], 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = train.merge(train_more, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_more = pd.read_csv('../data/test.csv')\n",
    "test_more.drop(['label', 'offer_value', 'offer_quantity'], 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = test.merge(test_more, on='id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_schema = StructType([\n",
    "    StructField(\"id\", LongType(), False),\n",
    "    StructField(\"offer_chain\", LongType(), True),\n",
    "    StructField(\"offer\", LongType(), True),\n",
    "    StructField(\"market\", LongType(), True),\n",
    "    StructField(\"repeattrips\", LongType(), True),\n",
    "    StructField(\"repeater\", IntegerType(), True),\n",
    "    StructField(\"offerdate\", StringType(), True),\n",
    "    StructField(\"offer_category\", LongType(), True),\n",
    "    StructField(\"quantity\", LongType(), True),\n",
    "    StructField(\"offer_company\", LongType(), True),\n",
    "    StructField(\"offervalue\", DoubleType(), True),\n",
    "    StructField(\"offer_brand\", LongType(), True),\n",
    "    StructField(\"offer_dept\", LongType(), True),\n",
    "    StructField(\"total_spend\", DoubleType(), True),\n",
    "    StructField(\"day_of_week\", DoubleType(), True),\n",
    "    StructField(\"day_of_month\", DoubleType(), True),\n",
    "    StructField(\"month\", DoubleType(), True),\n",
    "    StructField(\"has_bought_company\", DoubleType(), True),\n",
    "    StructField(\"has_bought_company_q\", DoubleType(), True),\n",
    "    StructField(\"has_bought_company_a\", DoubleType(), True),\n",
    "    StructField(\"has_bought_company_30\", DoubleType(), True),\n",
    "    StructField(\"has_bought_company_q_30\", DoubleType(), True),\n",
    "    StructField(\"has_bought_company_a_30\", DoubleType(), True),\n",
    "    StructField(\"has_bought_company_60\", DoubleType(), True),\n",
    "    StructField(\"has_bought_company_q_60\", DoubleType(), True),\n",
    "    StructField(\"has_bought_company_a_60\", DoubleType(), True),\n",
    "    StructField(\"has_bought_company_90\", DoubleType(), True),\n",
    "    StructField(\"has_bought_company_q_90\", DoubleType(), True),\n",
    "    StructField(\"has_bought_company_a_90\", DoubleType(), True),\n",
    "    StructField(\"has_bought_company_180\", DoubleType(), True),\n",
    "    StructField(\"has_bought_company_q_180\", DoubleType(), True),\n",
    "    StructField(\"has_bought_company_a_180\", DoubleType(), True),\n",
    "    StructField(\"has_bought_category\", DoubleType(), True),\n",
    "    StructField(\"has_bought_category_q\", DoubleType(), True),\n",
    "    StructField(\"has_bought_category_a\", DoubleType(), True),\n",
    "    StructField(\"has_bought_category_30\", DoubleType(), True),\n",
    "    StructField(\"has_bought_category_q_30\", DoubleType(), True),\n",
    "    StructField(\"has_bought_category_a_30\", DoubleType(), True),\n",
    "    StructField(\"has_bought_category_60\", DoubleType(), True),\n",
    "    StructField(\"has_bought_category_q_60\", DoubleType(), True),\n",
    "    StructField(\"has_bought_category_a_60\", DoubleType(), True),\n",
    "    StructField(\"has_bought_category_90\", DoubleType(), True),\n",
    "    StructField(\"has_bought_category_q_90\", DoubleType(), True),\n",
    "    StructField(\"has_bought_category_a_90\", DoubleType(), True),\n",
    "    StructField(\"has_bought_category_180\", DoubleType(), True),\n",
    "    StructField(\"has_bought_category_q_180\", DoubleType(), True),\n",
    "    StructField(\"has_bought_category_a_180\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_q\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_a\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_30\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_q_30\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_a_30\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_60\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_q_60\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_a_60\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_90\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_q_90\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_a_90\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_180\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_q_180\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_a_180\", DoubleType(), True),\n",
    "    StructField(\"never_bought_company\", DoubleType(), True),\n",
    "    StructField(\"never_bought_category\", DoubleType(), True),\n",
    "    StructField(\"never_bought_brand\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_company_category\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_category\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_company\", DoubleType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = sqlContext.createDataFrame(train, train_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_schema = StructType([\n",
    "    StructField(\"id\", LongType(), False),\n",
    "    StructField(\"offer_chain\", LongType(), True),\n",
    "    StructField(\"offer\", LongType(), True),\n",
    "    StructField(\"market\", LongType(), True),\n",
    "    StructField(\"offerdate\", StringType(), True),\n",
    "    StructField(\"offer_category\", LongType(), True),\n",
    "    StructField(\"quantity\", LongType(), True),\n",
    "    StructField(\"offer_company\", LongType(), True),\n",
    "    StructField(\"offervalue\", DoubleType(), True),\n",
    "    StructField(\"offer_brand\", LongType(), True),\n",
    "    StructField(\"offer_dept\", LongType(), True),\n",
    "    StructField(\"total_spend\", DoubleType(), True),\n",
    "    StructField(\"day_of_week\", DoubleType(), True),\n",
    "    StructField(\"day_of_month\", DoubleType(), True),\n",
    "    StructField(\"month\", DoubleType(), True),\n",
    "    StructField(\"has_bought_company\", DoubleType(), True),\n",
    "    StructField(\"has_bought_company_q\", DoubleType(), True),\n",
    "    StructField(\"has_bought_company_a\", DoubleType(), True),\n",
    "    StructField(\"has_bought_company_30\", DoubleType(), True),\n",
    "    StructField(\"has_bought_company_q_30\", DoubleType(), True),\n",
    "    StructField(\"has_bought_company_a_30\", DoubleType(), True),\n",
    "    StructField(\"has_bought_company_60\", DoubleType(), True),\n",
    "    StructField(\"has_bought_company_q_60\", DoubleType(), True),\n",
    "    StructField(\"has_bought_company_a_60\", DoubleType(), True),\n",
    "    StructField(\"has_bought_company_90\", DoubleType(), True),\n",
    "    StructField(\"has_bought_company_q_90\", DoubleType(), True),\n",
    "    StructField(\"has_bought_company_a_90\", DoubleType(), True),\n",
    "    StructField(\"has_bought_company_180\", DoubleType(), True),\n",
    "    StructField(\"has_bought_company_q_180\", DoubleType(), True),\n",
    "    StructField(\"has_bought_company_a_180\", DoubleType(), True),\n",
    "    StructField(\"has_bought_category\", DoubleType(), True),\n",
    "    StructField(\"has_bought_category_q\", DoubleType(), True),\n",
    "    StructField(\"has_bought_category_a\", DoubleType(), True),\n",
    "    StructField(\"has_bought_category_30\", DoubleType(), True),\n",
    "    StructField(\"has_bought_category_q_30\", DoubleType(), True),\n",
    "    StructField(\"has_bought_category_a_30\", DoubleType(), True),\n",
    "    StructField(\"has_bought_category_60\", DoubleType(), True),\n",
    "    StructField(\"has_bought_category_q_60\", DoubleType(), True),\n",
    "    StructField(\"has_bought_category_a_60\", DoubleType(), True),\n",
    "    StructField(\"has_bought_category_90\", DoubleType(), True),\n",
    "    StructField(\"has_bought_category_q_90\", DoubleType(), True),\n",
    "    StructField(\"has_bought_category_a_90\", DoubleType(), True),\n",
    "    StructField(\"has_bought_category_180\", DoubleType(), True),\n",
    "    StructField(\"has_bought_category_q_180\", DoubleType(), True),\n",
    "    StructField(\"has_bought_category_a_180\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_q\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_a\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_30\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_q_30\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_a_30\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_60\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_q_60\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_a_60\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_90\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_q_90\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_a_90\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_180\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_q_180\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_a_180\", DoubleType(), True),\n",
    "    StructField(\"never_bought_company\", DoubleType(), True),\n",
    "    StructField(\"never_bought_category\", DoubleType(), True),\n",
    "    StructField(\"never_bought_brand\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_company_category\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_category\", DoubleType(), True),\n",
    "    StructField(\"has_bought_brand_company\", DoubleType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = sqlContext.createDataFrame(test, test_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function converts the string cell into a date:\n",
    "stringToDate = F.udf(lambda x: datetime.strptime(x, '%Y-%m-%d'), DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert offerdate to date type\n",
    "train_df = train_df.withColumn('offerdate', stringToDate(F.col('offerdate')))\n",
    "test_df = test_df.withColumn('offerdate', stringToDate(F.col('offerdate')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = train_df.withColumnRenamed('repeater', 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get number of refund transactions and total transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------+--------------------+------------------+\n",
      "|      id|total_trans|total_returns|avg_purchasequantity|avg_purchaseamount|\n",
      "+--------+-----------+-------------+--------------------+------------------+\n",
      "|18470775|        350|            8|                1.34|3.1511428571428635|\n",
      "|14723452|        755|            5|  1.2264900662251657| 4.073933774834406|\n",
      "|15738658|         39|            0|   1.205128205128205| 4.780000000000001|\n",
      "|17552659|        591|            4|  1.5245346869712353| 4.931455160744474|\n",
      "|12996040|        326|            5|  1.2331288343558282|4.7280981595092095|\n",
      "|16078766|        966|           54|   1.818840579710145| 5.298498964803263|\n",
      "|18249735|       1557|           60|  1.2594733461785486|  4.13752087347458|\n",
      "|14989775|        614|           39|   1.231270358306189|3.8022475570032475|\n",
      "|15073302|        526|           38|  1.3669201520912548| 4.640133079847899|\n",
      "|16075389|        591|           32|  1.9593908629441625| 7.553604060913664|\n",
      "|16606739|        678|            6|  1.2905604719764012|5.6328908554571955|\n",
      "|15705695|        431|            8|   1.357308584686775| 5.145730858468677|\n",
      "|14576147|        817|           66|  1.5960832313341493|3.6231578947368455|\n",
      "|15134033|        944|           13|  1.7997881355932204| 5.452923728813514|\n",
      "|16551772|       1699|           47|  1.4096527369040612| 4.735797527957543|\n",
      "|17652157|       1407|           15|  1.2224591329068941| 4.885799573560684|\n",
      "|13089312|       1218|           52|  1.2060755336617406| 3.194318555008175|\n",
      "|13744500|       2232|          112|   1.260304659498208| 4.279466845878071|\n",
      "|16829614|        738|           10|  1.2357723577235773|3.0218834688346874|\n",
      "|17524817|        328|            7|  1.3567073170731707|3.6432926829268335|\n",
      "+--------+-----------+-------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "id_stats = transactions.select('id', 'purchasequantity', 'purchaseamount',\n",
    "                              F.when(transactions['purchaseamount'] < 0, 1).otherwise(0).alias('return'))\\\n",
    "                      .withColumn('1', F.lit(1))\n",
    "id_stats = id_stats.groupBy('id').agg(F.sum('1').alias('total_trans'), \n",
    "                                      F.sum('return').alias('total_returns'),\n",
    "                                      F.avg('purchasequantity').alias('avg_purchasequantity'),\n",
    "                                      F.avg('purchaseamount').alias('avg_purchaseamount'))\n",
    "id_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rename id column of returns table to avoid ambiguous column names\n",
    "id_stats = id_stats.withColumnRenamed('id', 'returns_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = train_df.join(id_stats, train_df['id'] == id_stats['returns_id'], 'left')\n",
    "test_df = test_df.join(id_stats, test_df['id'] == id_stats['returns_id'], 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = train_df.drop('repeattrips', 'quantity', 'returns_id')\n",
    "test_df = test_df.drop('quantity', 'returns_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get average value for imputations\n",
    "avg_total_trans = id_stats.agg(F.avg(F.col('total_trans'))).first()[0]\n",
    "avg_total_returns = id_stats.agg(F.avg(F.col('total_returns'))).first()[0]\n",
    "avg_avg_purchasequantity = id_stats.agg(F.avg(F.col('avg_purchasequantity'))).first()[0]\n",
    "avg_avg_purchaseamount = id_stats.agg(F.avg(F.col('avg_purchaseamount'))).first()[0]\n",
    "\n",
    "# Impute missing data\n",
    "impute_dict = {'total_trans': avg_total_trans, 'total_returns': avg_total_returns,\n",
    "               'avg_purchasequantity': avg_avg_purchasequantity,\n",
    "               'avg_purchaseamount': avg_avg_purchaseamount}\n",
    "train_df = train_df.fillna(impute_dict)\n",
    "test_df = test_df.fillna(impute_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get number of transactions that match the offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_trans_agg = train_df.join(transactions, \n",
    "                            (train_df.id == transactions.id) & (train_df.offer_chain == transactions.chain), \n",
    "                            \"left\") \\\n",
    "                          .select(train_df[\"id\"], \"offer_dept\", \"dept\", \"offer_company\", \"company\", \n",
    "                                  \"offer_brand\", \"brand\", \"offer_category\", \"category\") \\\n",
    "                          .withColumn(\"matchDeptCount\", F.expr(\"case when offer_dept = dept then 1 else 0 end\")) \\\n",
    "                          .withColumn(\"matchCategoryCount\", F.expr(\"case when offer_category = category then 1 else 0 end\")) \\\n",
    "                          .withColumn(\"matchCompanyCount\", F.expr(\"case when offer_company = company then 1 else 0 end\")) \\\n",
    "                          .withColumn(\"matchBrandCount\", F.expr(\"case when offer_brand = brand then 1 else 0 end\")) \\\n",
    "                          .groupBy(\"id\") \\\n",
    "                          .agg(F.sum(\"matchDeptCount\").alias(\"matchDeptCount\"),\n",
    "                               F.sum(\"matchCategoryCount\").alias(\"matchCategoryCount\"),\n",
    "                               F.sum(\"matchCompanyCount\").alias(\"matchCompanyCount\"),\n",
    "                               F.sum(\"matchBrandCount\").alias(\"matchBrandCount\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = train_df.join(train_trans_agg, train_df.id == train_trans_agg.id, \"left\").drop(train_trans_agg.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_trans_agg = test_df.join(transactions, \n",
    "                            (test_df.id == transactions.id) & (test_df.offer_chain == transactions.chain), \n",
    "                            \"left\") \\\n",
    "                          .select(test_df[\"id\"], \"offer_dept\", \"dept\", \"offer_company\", \"company\", \n",
    "                                  \"offer_brand\", \"brand\", \"offer_category\", \"category\") \\\n",
    "                          .withColumn(\"matchDeptCount\", F.expr(\"case when offer_dept = dept then 1 else 0 end\")) \\\n",
    "                          .withColumn(\"matchCategoryCount\", F.expr(\"case when offer_category = category then 1 else 0 end\")) \\\n",
    "                          .withColumn(\"matchCompanyCount\", F.expr(\"case when offer_company = company then 1 else 0 end\")) \\\n",
    "                          .withColumn(\"matchBrandCount\", F.expr(\"case when offer_brand = brand then 1 else 0 end\")) \\\n",
    "                          .groupBy(\"id\") \\\n",
    "                          .agg(F.sum(\"matchDeptCount\").alias(\"matchDeptCount\"),\n",
    "                               F.sum(\"matchCategoryCount\").alias(\"matchCategoryCount\"),\n",
    "                               F.sum(\"matchCompanyCount\").alias(\"matchCompanyCount\"),\n",
    "                               F.sum(\"matchBrandCount\").alias(\"matchBrandCount\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = test_df.join(test_trans_agg, test_df.id == test_trans_agg.id, \"left\").drop(test_trans_agg.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = train_df.na.fill(0)\n",
    "test_df = test_df.na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = train_df.drop('offerdate')\n",
    "test_df = test_df.drop('offerdate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Create pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get all the offers, categories, brands and companies from both training and test\n",
    "# Then create a StringIndexer to map categorical values to numeric labels\n",
    "cols_to_index = ['offer', 'offer_category', 'offer_brand', 'offer_company',\n",
    "                 'never_bought_company', 'never_bought_category', 'never_bought_brand',\n",
    "                 'has_bought_brand_company_category', 'has_bought_brand_category', 'has_bought_brand_company']\n",
    "indexers = []\n",
    "for col in cols_to_index:\n",
    "    union_df = train_df.select(col).union(test_df.select(col))\n",
    "    indexers.append(StringIndexer().setInputCol(col).setOutputCol(col+'idx').fit(union_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# One-hot-encoding of categorical variables\n",
    "encoders = []\n",
    "for col in cols_to_index:\n",
    "    encoders.append(OneHotEncoder().setInputCol(col+'idx').setOutputCol(col+'_encoded').setDropLast(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assemble features\n",
    "inputCols = [col for col in train_df.columns if col != 'id' or col != 'label']\n",
    "assembler = VectorAssembler(inputCols=inputCols, outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "lr = LogisticRegression(maxIter=10, standardization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "stages = indexers + encoders + [assembler, lr]\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regularization = [0.0001, 0.001]   # for testing purposes\n",
    "#regularization = [0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5]\n",
    "kfolds = 2\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, regularization) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=kfolds)  # use 3+ folds in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[offer_chain: bigint, offer: bigint, market: bigint, offer_category: bigint, offer_company: bigint, offervalue: double, offer_brand: bigint, offer_dept: bigint, total_spend: double, day_of_week: double, day_of_month: double, month: double, has_bought_company: double, has_bought_company_q: double, has_bought_company_a: double, has_bought_company_30: double, has_bought_company_q_30: double, has_bought_company_a_30: double, has_bought_company_60: double, has_bought_company_q_60: double, has_bought_company_a_60: double, has_bought_company_90: double, has_bought_company_q_90: double, has_bought_company_a_90: double, has_bought_company_180: double, has_bought_company_q_180: double, has_bought_company_a_180: double, has_bought_category: double, has_bought_category_q: double, has_bought_category_a: double, has_bought_category_30: double, has_bought_category_q_30: double, has_bought_category_a_30: double, has_bought_category_60: double, has_bought_category_q_60: double, has_bought_category_a_60: double, has_bought_category_90: double, has_bought_category_q_90: double, has_bought_category_a_90: double, has_bought_category_180: double, has_bought_category_q_180: double, has_bought_category_a_180: double, has_bought_brand: double, has_bought_brand_q: double, has_bought_brand_a: double, has_bought_brand_30: double, has_bought_brand_q_30: double, has_bought_brand_a_30: double, has_bought_brand_60: double, has_bought_brand_q_60: double, has_bought_brand_a_60: double, has_bought_brand_90: double, has_bought_brand_q_90: double, has_bought_brand_a_90: double, has_bought_brand_180: double, has_bought_brand_q_180: double, has_bought_brand_a_180: double, never_bought_company: double, never_bought_category: double, never_bought_brand: double, has_bought_brand_company_category: double, has_bought_brand_category: double, has_bought_brand_company: double, total_trans: bigint, total_returns: bigint, avg_purchasequantity: double, avg_purchaseamount: double, id: bigint, matchDeptCount: bigint, matchCategoryCount: bigint, matchCompanyCount: bigint, matchBrandCount: bigint]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.cache()\n",
    "test_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvModel = crossval.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.00021268621136,8.52570830079e-05,-0.00360507936997,8.25132282822,0.000168524554689,1.29870264487e-09,-0.311250743646,1.73977822604e-06,0.0168459261006,3.71170153568e-06,-0.0148926841412,-0.00717350161858,0.909008971754,0.000766168517679,0.000192083813055,0.000140905817756,0.0513684788081,0.0084997328229,0.00681638103274,0.0189044703092,0.00314103892444,0.00264817686979,0.0105846511406,0.00166479252982,0.00160083901208,0.00308900513662,0.000547425180504,0.000575256998085,0.00710364117571,0.00121889197032,0.000423134280496,0.0682267049869,0.0222819797416,0.00458415850766,0.0295162770439,0.00688105577181,0.00131771371859,0.0205751093733,0.00543957958506,0.00113006462949,0.0120531768131,0.00317615797174,0.000682246586395,0.00523636672102,0.00136888058705,0.000577888954771,0.140064362997,0.0303953874848,0.0140175962607,0.0609542566711,0.0141912751971,0.00592389279545,0.0392390969293,0.00968538013675,0.00407642562136,0.0162148501202,0.00435397553599,0.00175411715114,0.0419879042769,-0.931286428956,-0.869309507855,1.21048496853,1.12604980329,0.925512038178,0.000275823235159,-0.0313666094678,-61.9261002693,-6.09702335182,1.04869506413e-10,0.0175414628252,0.00165726142381,0.0601052953322,0.0139796710107]\n",
      "Intercept: -0.987979613109\n"
     ]
    }
   ],
   "source": [
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(cvModel.bestModel.stages[-1].coefficients))\n",
    "print(\"Intercept: \" + str(cvModel.bestModel.stages[-1].intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting on training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pred = cvModel.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|         probability|prediction|\n",
      "+--------------------+----------+\n",
      "|[0.99961210929115...|       0.0|\n",
      "|[0.99984913889841...|       0.0|\n",
      "|[0.36965916464421...|       1.0|\n",
      "|[0.99718165945590...|       0.0|\n",
      "|[0.99969608317672...|       0.0|\n",
      "|[0.99986816816284...|       0.0|\n",
      "|[0.99961446439226...|       0.0|\n",
      "|[0.99960767338824...|       0.0|\n",
      "|[0.99984992055939...|       0.0|\n",
      "|[0.65505132003607...|       0.0|\n",
      "|[0.99985809084751...|       0.0|\n",
      "|[0.99985196054892...|       0.0|\n",
      "|[0.99984768171038...|       0.0|\n",
      "|[0.09065252490991...|       1.0|\n",
      "|[4.29971472427468...|       1.0|\n",
      "|[0.53250963237168...|       0.0|\n",
      "|[0.50161725196770...|       0.0|\n",
      "|[0.99986320385462...|       0.0|\n",
      "|[7.17447161655919...|       1.0|\n",
      "|[0.99985328741319...|       0.0|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_pred.select('probability', 'prediction').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "u'Field \"label\" does not exist.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-f2eb0fccc0c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcvModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/ml/base.pyc\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/ml/tuning.pyc\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbestModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1.4.0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/ml/base.pyc\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/ml/pipeline.pyc\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/ml/base.pyc\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/ml/wrapper.pyc\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: u'Field \"label\" does not exist.'"
     ]
    }
   ],
   "source": [
    "test_pred = cvModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_pred.select('probability', 'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_id = test_df.select('id').toPandas()\n",
    "test_prob = test_pred.select('probability').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I didn't know how to access the vector inside the probability column.\n",
    "# To work around this, I looped through the probability column and used the indexing operations to pull out\n",
    "# the individual probabilities.\n",
    "prob = []\n",
    "for i in range(test_prob.shape[0]):\n",
    "    prob.append(test_prob['probability'][i][1])\n",
    "test_prob['repeatProbability'] = prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_final_pred = pd.merge(test_id, test_prob, left_index=True, right_index=True)\n",
    "test_final_pred.drop('probability', 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_final_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_final_pred['repeatProbability'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write the test file to csv\n",
    "test_final_pred.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
